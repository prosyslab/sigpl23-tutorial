{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeonhee-ryou/sigpl23-tutorial/blob/main/2_Fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 환경설정\n",
        "\n",
        "* 라이브러리 설치\n",
        "* 구글 드라이브 마운트\n",
        "* 텐서보드 연결"
      ],
      "metadata": {
        "id": "w4tT935RvE9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets evaluate accelerate"
      ],
      "metadata": {
        "id": "6GXN_t-vuws5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "home_dir = \"/content/gdrive/MyDrive/Colab-Data\"\n",
        "model_dir = f\"{home_dir}/models/codebert-refinement\""
      ],
      "metadata": {
        "id": "zydl0qJb23HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "yW4EgYeBdP6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir $model_dir"
      ],
      "metadata": {
        "id": "n702os_ddzws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CodeBERT Fine-tuning 학습하기"
      ],
      "metadata": {
        "id": "YUrKSySiU4PY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 토크나이저, 데이터셋, 사전학습된 CodeBERT 모델 준비\n",
        "\n",
        "#### `RobertaLMHeadModel` 모델 구조\n",
        "* Roberta 모델 + Causal Language Model 구조 사용\n",
        "* Embedding Layer + 12 x Encoder Layer + Pooler Layer\n",
        "  * Embedding Layer: batch_size * 514 * 50,265 -> batch_size * 514 * 768\n",
        "  * Encoder Layer: batch_size * 514 * 768 -> batch_size * 514 * 768\n",
        "* LM Layer: batch_size * 514 * 768 -> batch_size * 514"
      ],
      "metadata": {
        "id": "eo5PdwYneemy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "sUbMUZJnU4PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akwH1LRoU4PZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/codebert-base\", is_decoder=True)\n",
        "print(repr(model))"
      ],
      "metadata": {
        "id": "WqeM7ry0U4PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"code_x_glue_cc_code_refinement\", \"small\")"
      ],
      "metadata": {
        "id": "XeSJWmiUU4Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"train\"][0]"
      ],
      "metadata": {
        "id": "wz-gjYu3Hddl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 전처리"
      ],
      "metadata": {
        "id": "F0F2WRm_Hy9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(examples):\n",
        "  tokenized_inputs = tokenizer(examples[\"buggy\"], padding=\"max_length\", truncation=True)\n",
        "  labels = tokenizer(examples[\"fixed\"], padding=\"max_length\", truncation=True).input_ids\n",
        "  return dict(labels=labels, **tokenized_inputs)\n",
        "\n",
        "tokenized_datasets = ds.map(tokenize, batched=True)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "da-rdFvYU4Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 샘플 데이터 준비"
      ],
      "metadata": {
        "id": "kbkPVrGienmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "sample = dict()\n",
        "sample_ratio = 0.01\n",
        "for split in tokenized_datasets:\n",
        "  size = round(tokenized_datasets[split].num_rows * sample_ratio)\n",
        "  sample[split] = tokenized_datasets[split].shuffle(seed=1234).select(range(size))\n",
        "\n",
        "sample_datasets = DatasetDict(sample)\n",
        "sample_datasets.num_rows"
      ],
      "metadata": {
        "id": "L7QlbbQ5U4Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_datasets[\"train\"][0].keys()"
      ],
      "metadata": {
        "id": "fRFKQoDsMXvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 학습 설정 정의\n",
        "* 평가식\n",
        "* Hyperparameters"
      ],
      "metadata": {
        "id": "n88pqmFVeqOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "  preds, labels = eval_preds\n",
        "  preds_ids = np.argmax(preds, axis=-1)\n",
        "  decoded_preds = tokenizer.batch_decode(preds_ids, skip_special_tokens=True)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "  res = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "  return {\"bleu\": res[\"bleu\"]}"
      ],
      "metadata": {
        "id": "0rNsSwObh-ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "  output_dir=f\"{model_dir}/sampled\",\n",
        "  evaluation_strategy=\"epoch\",\n",
        "  save_strategy=\"epoch\",\n",
        "  num_train_epochs=3.0,\n",
        "  per_device_train_batch_size=8,\n",
        "  per_device_eval_batch_size =16,\n",
        "  learning_rate=2e-5,\n",
        "  lr_scheduler_type=\"linear\",\n",
        "  warmup_ratio=0.1,\n",
        "  logging_steps=10\n",
        ")\n"
      ],
      "metadata": {
        "id": "OhKOZWTgJ_Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 샘플 데이터에서 학습해보기"
      ],
      "metadata": {
        "id": "zMcXwUUle6t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "  model=model,\n",
        "  args=args,\n",
        "  train_dataset=sample_datasets[\"train\"],\n",
        "  eval_dataset=sample_datasets[\"validation\"],\n",
        "  compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train(resume_from_checkpoint=None)\n",
        "trainer.save_model(args.output_dir)"
      ],
      "metadata": {
        "id": "ckIVl4OKU4Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 전체 데이터에서 학습하기"
      ],
      "metadata": {
        "id": "FFN81jYBe_zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "  output_dir=f\"{model_dir}/full\",\n",
        "  evaluation_strategy=\"epoch\",\n",
        "  save_strategy=\"epoch\",\n",
        "  num_train_epochs=5.0,\n",
        "  per_device_train_batch_size=8,\n",
        "  per_device_eval_batch_size =16,\n",
        "  learning_rate=2e-5,\n",
        "  lr_scheduler_type=\"linear\",\n",
        "  warmup_ratio=0.1,\n",
        "  logging_steps=20,\n",
        "  seed=1234,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "  model=model,\n",
        "  args=args,\n",
        "  train_dataset=tokenized_datasets['train'],\n",
        "  eval_dataset=tokenized_datasets['validation'],\n",
        "  compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train(resume_from_checkpoint=None)\n",
        "trainer.save_model(args.output_dir)"
      ],
      "metadata": {
        "id": "zsE_mVkSU4Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 테스트 데이터에서 정확도 검토하기\n",
        "\n",
        "※ 참고: CodeXGLUE 리더보드 https://microsoft.github.io/CodeXGLUE/"
      ],
      "metadata": {
        "id": "cnTpR1_Q0leA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(sample_datasets[\"test\"])"
      ],
      "metadata": {
        "id": "-JWLFTF11qa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "predicts = []\n",
        "labels = []\n",
        "ds_test = sample_datasets[\"test\"]\n",
        "for batch in DataLoader(ds_test, batch_size=32):\n",
        "  input_ids = torch.stack(batch[\"input_ids\"], dim=1).to(device)\n",
        "  attention_mask = torch.stack(batch[\"attention_mask\"], dim=1).to(device)\n",
        "  with torch.no_grad():\n",
        "    model_out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    batch_preds = tokenizer.batch_decode(torch.argmax(model_out.logits, dim=-1).detach(), skip_special_tokens=True)\n",
        "  batch_labels = tokenizer.batch_decode(torch.stack(batch[\"labels\"], dim=-1), skip_special_tokens=True)\n",
        "  predicts.extend(batch_preds)\n",
        "  labels.extend(batch_labels)\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "bleu.compute(predictions=predicts, references=labels)"
      ],
      "metadata": {
        "id": "ZhPEPth-9AbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 저장된 모델 읽어서 실행해보기"
      ],
      "metadata": {
        "id": "asSPyo9w7Aq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import RobertaForCausalLM\n",
        "\n",
        "ds_test = tokenized_datasets['test']\n",
        "\n",
        "home_dir = \"/content/gdrive/MyDrive/Colab-Data\"\n",
        "\n",
        "model = RobertaForCausalLM.from_pretrained(f\"{model_dir}/full\")\n",
        "model.eval()\n",
        "model.to(torch.device('cuda'))"
      ],
      "metadata": {
        "id": "EnkjbWZ30jcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')\n",
        "ex = ds_test[0]\n",
        "input_data = dict(\n",
        "  input_ids=torch.tensor([ex[\"input_ids\"]]).to(device),\n",
        "  attention_mask=torch.tensor([ex[\"attention_mask\"]]).to(device)\n",
        ")\n",
        "model_out = model(**input_data)\n",
        "ex[\"target\"], tokenizer.batch_decode(torch.argmax(model_out.logits, dim=-1).detach(), skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "XPj9lB7W1GdG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}