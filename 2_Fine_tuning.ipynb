{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prosyslab/sigpl23-tutorial/blob/main/2_Fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 환경설정\n",
        "\n",
        "* 라이브러리 설치\n",
        "* 구글 드라이브 마운트\n",
        "* 텐서보드 연결"
      ],
      "metadata": {
        "id": "w4tT935RvE9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets evaluate accelerate"
      ],
      "metadata": {
        "id": "6GXN_t-vuws5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "home_dir = \"/content/gdrive/MyDrive/Colab-Data\"\n",
        "model_dir = f\"{home_dir}/models/codebert-code-completion\""
      ],
      "metadata": {
        "id": "zydl0qJb23HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "yW4EgYeBdP6e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir $model_dir"
      ],
      "metadata": {
        "id": "n702os_ddzws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CodeBERT Fine-tuning 학습하기"
      ],
      "metadata": {
        "id": "YUrKSySiU4PY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 토크나이저, 데이터셋, 사전학습된 CodeBERT 모델 준비\n",
        "\n",
        "#### `RobertaLMHeadModel` 모델 구조\n",
        "* Roberta 모델 + Causal Language Model 구조 사용\n",
        "* Embedding Layer + 12 x Encoder Layer + Pooler Layer\n",
        "  * Embedding Layer: batch_size * 514 * 50,265 -> batch_size * 514 * 768\n",
        "  * Encoder Layer: batch_size * 514 * 768 -> batch_size * 514 * 768\n",
        "* LM Layer: batch_size * 514 * 768 -> batch_size * 514"
      ],
      "metadata": {
        "id": "eo5PdwYneemy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "sUbMUZJnU4PZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "akwH1LRoU4PZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\", add_prefix_space=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/codebert-base\", is_decoder=True)\n",
        "print(repr(model))"
      ],
      "metadata": {
        "id": "WqeM7ry0U4PZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c6a54a-4bdc-4759-bc41-a9ff472180bc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobertaForCausalLM(\n",
            "  (roberta): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): RobertaLMHead(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"code_x_glue_cc_code_completion_token\", \"java\")"
      ],
      "metadata": {
        "id": "XeSJWmiUU4Pa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 전처리"
      ],
      "metadata": {
        "id": "F0F2WRm_Hy9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(examples):\n",
        "  tokenized_inputs = tokenizer(examples[\"code\"], padding=\"max_length\", truncation=True, is_split_into_words=True, add_special_tokens=False)\n",
        "  labels = tokenized_inputs.input_ids\n",
        "  return dict(labels=labels, **tokenized_inputs)\n",
        "\n",
        "tokenized_datasets = ds.map(tokenize, batched=True)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "da-rdFvYU4Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 샘플 데이터 준비"
      ],
      "metadata": {
        "id": "kbkPVrGienmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "sample = dict()\n",
        "sample_ratio = 0.1\n",
        "size = round(tokenized_datasets[\"train\"].num_rows * sample_ratio)\n",
        "sample[\"train\"] = tokenized_datasets[\"train\"].shuffle(seed=1234).select(range(size))\n",
        "\n",
        "\"\"\"validation, test 에대해서도 동일한 방법으로 데이터를 샘플링할 수 있습니다\"\"\"\n",
        "\n",
        "sample_datasets = DatasetDict(sample)\n",
        "sample_datasets.num_rows"
      ],
      "metadata": {
        "id": "L7QlbbQ5U4Pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d14ff0-1823-48cd-9010-2bda999ea7d5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': 1293}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 학습 설정\n"
      ],
      "metadata": {
        "id": "n88pqmFVeqOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyper parameters\n",
        "실습에서 사용하는 하이퍼파라미터 외에도 실제로 사용되는 하이퍼파라미터가 많습니다. [공식 문서](https://huggingface.co/docs/transformers/main_classes/trainer)를 참고하세요.\n",
        "\n",
        "* `output_dir`: 모델 저장 위치. 체크포인트, 로그 등 저장\n",
        "* `evaluation_strategy`: 학습 중 `eval_dataset` 을 이용해서 평가하는 단위\n",
        "* `save_strategy`: 저장 단위\n",
        "* `num_train_epochs`: 총 데이터 학습 횟수 지정\n",
        "* `per_device_train_batch_size`: 학습 데이터 배치 크기. 이 크기에 따라 메모리 사용량이 매우 달라집니다.\n",
        "* `gradient_accumulation_steps`: 역전파 단위\n",
        "  * `per_device_train_batch_size * gradient_accumulation_steps` 단위로 역전파되며, 이 크기를 총 `TOTAL_BATCH_SIZE` 라고 부르기도 합니다. 분산 학습일 경우에는 사용하는 GPU 갯수까지 곱해서 사용합니다.\n",
        "  * 하이퍼파라미터에서 `*_steps` 의이름으로 지정되는 값의 경우 1 step 의 크기는 `TOTAL_BATCH_SIZE` 입니다.\n",
        "* `per_device_eval_batch_size`: 학습 중 평가 시 사용하는 데이터 배치 크기.\n",
        "  * 평가 시에는 모델을 업데이트하지 않기 때문에 역전파를 위한 중간 텐서를 유지하지 않고, 따라서 메모리 사용량이 학습 과정에 비해 적습니다. 동일한 메모리를 사용할 때 학습 데이터 배치보다 평가 데이터 배치를 크게 잡을 수 있습니다.\n",
        "* `learning_rate`: gradient step 크기\n",
        "* `lr_scheduler_type`: 학습 중 learning rate 를 바꾸는 방법\n",
        "  * `\"linear\"`: 전체 학습 횟수에 도달할 때까지 선형으로 감소\n",
        "  * `\"constant\"`: 전체 학습 중 일정한 learning rate 유지\n",
        "  * `\"cosine\"`: consine 함수에 따라 learning rate 가 진동\n",
        "  * 이 외 문서 참고\n",
        "* `warmup_ratio`: 학습 초기에 전체 학습 횟수의 `warmup_ratio` 만큼 동안 지정한 `learning_rate` 까지 선형으로 증가하도록 설정\n",
        "  * 학습 초기에는 learning rate 가 너무 크면 로컬 옵티멈에 빠지기 쉽습니다.\n",
        "* `logging_steps`: loss 등의 학습 메트링을 로깅하는 단위\n",
        "  * `gradient_accumulation_steps * logging_steps` 만큼 데이터를 학습한 뒤 로깅합니다."
      ],
      "metadata": {
        "id": "9HKwDf7hgw2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "  output_dir=model_dir,\n",
        "  evaluation_strategy=\"epoch\",\n",
        "  save_strategy=\"epoch\",\n",
        "  num_train_epochs=3.0,\n",
        "  per_device_train_batch_size=8,\n",
        "  gradient_accumulation_steps=32,\n",
        "  per_device_eval_batch_size=16,\n",
        "  learning_rate=2e-5,\n",
        "  lr_scheduler_type=\"linear\",\n",
        "  warmup_ratio=0.1,\n",
        "  logging_steps=1,\n",
        "  seed=1234,\n",
        ")\n"
      ],
      "metadata": {
        "id": "OhKOZWTgJ_Qm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 샘플 데이터에서 학습해보기"
      ],
      "metadata": {
        "id": "zMcXwUUle6t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "  model=model,\n",
        "  args=args,\n",
        "  train_dataset=sample_datasets[\"train\"],      # 학습 데이터\n",
        "  eval_dataset=sample_datasets[\"validation\"],  # 평가 데이터\n",
        ")\n",
        "trainer.train(resume_from_checkpoint=None)\n",
        "trainer.save_model(args.output_dir)"
      ],
      "metadata": {
        "id": "ckIVl4OKU4Pb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "3bcc4b04-f7a1-4840-a68d-217a52e9a38e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 09:22, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>14.666900</td>\n",
              "      <td>14.090137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>11.568200</td>\n",
              "      <td>11.337207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>10.463000</td>\n",
              "      <td>10.389155</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 테스트 데이터에서 정확도 검토하기\n",
        "\n",
        "※ 참고: CodeXGLUE 리더보드 https://microsoft.github.io/CodeXGLUE/\n",
        "* CodeXGLUE 벤치마크 평가 시에는 생성 시간에 Beam search 가 적용되어있어 아래 평가식과는 차이가 있습니다.\n",
        "* Beam search 가 적용된 생성 품질을 평가 해보고 싶을 경우 CodeXGLUE 가 제공하는 평가 스크립트 사용해보세요\n",
        "* 일반적으로 Beam search 를 적용해서 Seq2Seq 모델을 학습하고 평가하고싶을 경우 `Seq2SeqTrainer` 를 사용할 수 있습니다.\n",
        "\n",
        "#### 평가식 정의\n",
        "* [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)\n",
        "* [bleu](https://huggingface.co/spaces/evaluate-metric/bleu)\n",
        "  * 정답과 예측 문자열이 비슷한 정도 측정"
      ],
      "metadata": {
        "id": "cnTpR1_Q0leA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "accuracy = evaluate.load(\"accuracy\")"
      ],
      "metadata": {
        "id": "0rNsSwObh-ET"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "predicts = []\n",
        "labels = []\n",
        "ds_test = sample_datasets[\"test\"].remove_columns(\"code\")\n",
        "for batch in DataLoader(ds_test, batch_size=32):\n",
        "  input_ids = torch.stack(batch[\"input_ids\"], dim=1).to(device)\n",
        "  attention_mask = torch.stack(batch[\"attention_mask\"], dim=1).to(device)\n",
        "  with torch.no_grad():\n",
        "    model_out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    batch_preds = torch.argmax(model_out.logits, dim=-1).detach()\n",
        "  batch_labels = torch.stack(batch[\"labels\"], dim=-1)\n",
        "  predicts.extend(batch_preds)\n",
        "  labels.extend(batch_labels)\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "print(accuracy.compute(predictions=torch.concat(predicts), references=torch.concat(labels)))\n",
        "\n",
        "predicts = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predicts]\n",
        "labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n",
        "print(bleu.compute(predictions=predicts, references=labels))\n"
      ],
      "metadata": {
        "id": "ZhPEPth-9AbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 저장된 모델 읽어서 실행해보기"
      ],
      "metadata": {
        "id": "asSPyo9w7Aq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import RobertaForCausalLM\n",
        "\n",
        "ds_test = tokenized_datasets['test']\n",
        "\n",
        "model = RobertaForCausalLM.from_pretrained(model_dir)\n",
        "model.eval()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "EnkjbWZ30jcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex = ds_test[0]\n",
        "model_out = model(\"\"\"모델 입력을 만들어 보세요\"\"\")\n",
        "tokenizer.batch_decode(torch.argmax(model_out.logits, dim=-1).detach(), skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "XPj9lB7W1GdG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}